# AIMA - Artificial Intelligence Media Analysis

![Status](https://img.shields.io/badge/Status-Development-orange)
![Implementation](https://img.shields.io/badge/Implementation-35%25-red)
![Architecture](https://img.shields.io/badge/Architecture-Microservices-blue)
![GPU](https://img.shields.io/badge/GPU-Cloud%20Based-green)
![License](https://img.shields.io/badge/License-Open%20Source-brightgreen)

## ğŸš¨ Aktueller Projektstatus

**âš ï¸ WICHTIGER HINWEIS: Dieses Projekt befindet sich in einer umfassenden Restrukturierung**

- âœ… **35% implementiert** - Grundlegende Frontend/Backend-Infrastruktur
- ğŸ”¶ **15% teilweise implementiert** - GPU-Management und Job-System (nur UI)
- âŒ **50% nicht implementiert** - Kern-ML-Pipeline und echte FunktionalitÃ¤t
- ğŸ”„ **VollstÃ¤ndige Neukonzeption erforderlich** - Aktuelle Implementation nicht produktionstauglich

## ğŸ“‹ ProjektÃ¼bersicht

AIMA ist ein **modulares, API-first ML/LLM-System** fÃ¼r intelligente Medienanalyse mit GPU-basierter Verarbeitung. Das System nutzt ausschlieÃŸlich **Open-Source-Modelle** und **selbst-gehostete GPU-Instanzen** fÃ¼r:

- ğŸ¥ **Video- und Fotoanalyse** mit KI-gestÃ¼tzter Objekterkennung
- ğŸ‘¤ **Personenerkennung** mit umfangreichem Dossier-System
- ğŸ—£ï¸ **Audio-Transkription** und Sprachanalyse
- ğŸ¤– **LLM-Integration** fÃ¼r kontextuelle Inhaltsanalyse
- â˜ï¸ **Cloud-GPU-Management** (RunPod, Vast.ai, Lambda Labs)

## ğŸ—ï¸ Architektur-Prinzipien

### Modulare Microservices-Architektur
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Core Services â”‚   ML Services   â”‚ Infrastructure  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ API Gateway   â”‚ â€¢ Face Recognitionâ”‚ â€¢ GPU Manager   â”‚
â”‚ â€¢ Auth Service  â”‚ â€¢ CLIP Analysis â”‚ â€¢ Storage Mgmt  â”‚
â”‚ â€¢ Job Queue     â”‚ â€¢ Whisper STT   â”‚ â€¢ Monitoring    â”‚
â”‚ â€¢ User Mgmt     â”‚ â€¢ LLM Engine    â”‚ â€¢ Database      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### GPU-Provider-Integration
- **RunPod**: Cloud-GPU-Instanzen fÃ¼r ML-Workloads
- **Vast.ai**: Dezentraler GPU-Marktplatz
- **Lambda Labs**: Spezialisierte ML-Hardware
- **Just-in-Time Provisioning**: Automatische GPU-Aktivierung nur bei Bedarf

## ğŸ¯ Kernfunktionen

### ğŸ‘¥ Intelligente Personenerkennung
- **Multi-modale Erkennung**: Gesicht, KÃ¶rper, Stimme, Kleidung
- **Automatische ID-Generierung**: TemporÃ¤re IDs (Person_001, Person_002)
- **Benutzer-editierbare Zuordnungen**: VollstÃ¤ndige Kontrolle Ã¼ber Personen-IDs
- **Umfangreiches Dossier-System**: Medien-Historie, Audio-Transkripte, Verhaltensmuster
- **Job-ID-Nachverfolgung**: VollstÃ¤ndige RÃ¼ckverfolgbarkeit aller Erkennungen

### ğŸ”„ Batch-Verarbeitung
- **Intelligente Job-Gruppierung**: Kostenoptimierte Batch-Verarbeitung
- **Automatische Ressourcenzuteilung**: Optimale GPU-Effizienz
- **PrioritÃ¤ten-System**: Dringende vs. Standard-Verarbeitung
- **Echtzeit-Ãœberwachung**: Live-Status und Kostentracking

### ğŸ¤– LLM-Integration
- **Uncensored Models**: Llama 3.1, Mistral, CodeLlama
- **Multi-modale Analyse**: Text, Bild, Video, Audio
- **Kontextuelle Interpretation**: Intelligente VerknÃ¼pfung von ML-Erkennungen
- **Narrative Generierung**: Automatische Berichtserstellung

### ğŸ—£ï¸ Audio-Engine
- **Mehrsprachige Transkription**: 100+ Sprachen mit Whisper
- **Sprecher-Identifikation**: Automatische Trennung verschiedener Sprecher
- **Emotionsanalyse**: KI-basierte Stimmungserkennung
- **Zeitstempel-Synchronisation**: PrÃ¤zise Audio-Text-Zuordnung

## ğŸ› ï¸ Technologie-Stack

### Backend
- **Node.js/TypeScript** - API-Services
- **PostgreSQL** - PrimÃ¤re Datenbank
- **Redis** - Caching und Job-Queue
- **Docker** - Containerisierung
- **Prisma** - Database ORM

### Frontend
- **React 18** - UI-Framework
- **TypeScript** - Type Safety
- **Tailwind CSS** - Styling
- **Zustand** - State Management
- **Vite** - Build Tool

### ML/AI Stack
- **PyTorch** - ML-Framework
- **CUDA** - GPU-Acceleration
- **FaceNet/ArcFace** - Gesichtserkennung
- **CLIP** - Multi-modale Analyse
- **Whisper** - Audio-Transkription
- **vLLM** - LLM-Inference

### Infrastructure
- **Docker Compose** - Orchestrierung
- **NGINX** - Reverse Proxy
- **Prometheus/Grafana** - Monitoring
- **MinIO** - Object Storage

## ğŸš€ Schnellstart (Entwicklung)

### Voraussetzungen
- Node.js 18+
- Docker & Docker Compose
- Git

### Installation
```bash
# Repository klonen
git clone https://github.com/your-org/AIMAS_AIDev.git
cd AIMAS_AIDev

# Dependencies installieren
npm install
cd server && npm install && cd ..

# Umgebungsvariablen konfigurieren
cp .env.example .env
cp server/.env.example server/.env

# Datenbank starten
docker-compose up -d postgres redis

# Backend starten
cd server && npm run dev

# Frontend starten (neues Terminal)
npm run dev
```

### Zugriff
- **Frontend**: http://localhost:5173
- **Backend API**: http://localhost:3001
- **Dokumentation**: http://localhost:3001/docs

## ğŸ“Š Implementierungsstatus

### âœ… VollstÃ¤ndig Implementiert (35%)
- Frontend-Grundstruktur (Dashboard, Navigation, Layout)
- Backend-Infrastruktur (API-Routen, Authentifizierung, WebSocket)
- Entwicklungsumgebung (Docker, Testing, CI/CD)

### ğŸ”¶ Teilweise Implementiert (15%)
- GPU-Management (UI vorhanden, keine echten Provider-APIs)
- Job-Management (UI vorhanden, keine Batch-Optimierung)
- Person-Dossiers (UI vorhanden, keine ML-Integration)

### âŒ Nicht Implementiert (50%)
- **Kern-ML-Pipeline**: Keine LLM-Integration
- **Spezialisierte ML-Tools**: FaceNet, CLIP, Whisper fehlen
- **Plugin-Architektur**: Keine modulare Erweiterbarkeit
- **Cloud-Integration**: Keine Dropbox/Mega-Anbindung
- **Audio-Engine**: Keine Whisper-Integration

## ğŸ—ºï¸ Roadmap

### Phase 1: Fundament (4-6 Wochen)
- [ ] Microservices-Architektur
- [ ] ML-Pipeline-Foundation
- [ ] GPU-Ressourcen-Scheduler
- [ ] Daten-Management-System

### Phase 2: ML-Integration (6-8 Wochen)
- [ ] FaceNet/ArcFace Gesichtserkennung
- [ ] CLIP Multi-modale Analyse
- [ ] Whisper Audio-Transkription
- [ ] LLM-Engine (Llama, Mistral)

### Phase 3: GPU-Provider (4-6 Wochen)
- [ ] RunPod API Integration
- [ ] Vast.ai API Integration
- [ ] Just-in-Time GPU Provisioning
- [ ] Cost Optimization

### Phase 4: Erweiterte Features (6-8 Wochen)
- [ ] Cloud-Integration (Dropbox, Mega)
- [ ] Plugin-System
- [ ] Advanced Analytics
- [ ] Timeline Analysis

## ğŸ¤ Beitragen

**âš ï¸ Hinweis**: Das Projekt befindet sich in einer umfassenden Restrukturierung. BeitrÃ¤ge sind willkommen, aber bitte beachten Sie den aktuellen Entwicklungsstatus.

### Entwicklung
1. Fork des Repositories
2. Feature-Branch erstellen (`git checkout -b feature/amazing-feature`)
3. Ã„nderungen committen (`git commit -m 'Add amazing feature'`)
4. Branch pushen (`git push origin feature/amazing-feature`)
5. Pull Request erstellen

### Code-QualitÃ¤t
- TypeScript Strict Mode
- ESLint + Prettier
- Unit Tests (Vitest)
- E2E Tests (Playwright)

## ğŸ“„ Lizenz

Dieses Projekt steht unter der MIT-Lizenz. Siehe [LICENSE](LICENSE) fÃ¼r Details.

## ğŸ”— Links

- **Dokumentation**: [VollstÃ¤ndige Restrukturierung](.trae/documents/AIMA_VollstÃ¤ndige_Restrukturierung.md)
- **PRD**: [ML Video Foto Analysesystem](.trae/documents/ML_Video_Foto_Analysesystem_PRD.md)
- **Implementierungsanalyse**: [PRD Umsetzungsanalyse](.trae/documents/AIMA_PRD_Umsetzungsanalyse.md)

## âš ï¸ Disclaimer

Dieses System ist fÃ¼r die Analyse von Medieninhalten konzipiert und nutzt uncensored ML/LLM-Modelle. Benutzer sind fÃ¼r die Einhaltung lokaler Gesetze und Vorschriften verantwortlich.

---

**Status**: ğŸ”„ In aktiver Entwicklung | **Letzte Aktualisierung**: Dezember 2024
